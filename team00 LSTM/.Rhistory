# 정확도 계산
accuracy <- mean(predicted_labels == test_data$Species)
cat("Accuracy:", accuracy, "\n")
# 실제 라벨
true_labels <- test_data$Species
# 정확도 계산
accuracy <- mean(predicted_labels == true_labels)
cat("Accuracy:", accuracy, "\n")
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model)
# LightGBM 모델 예제
library(lightgbm)
library(dplyr)
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model)
# 라벨 값 수정 (0, 1, 2로 변경)
iris$Species <- as.numeric(factor(iris$Species)) - 1
# 트레이닝 데이터와 테스트 데이터 분리
set.seed(123)
indices <- sample(1:nrow(iris), nrow(iris) * 0.7)
train_data <- iris[indices, ]
test_data <- iris[-indices, ]
# LightGBM 데이터셋으로 변환
train_data_lgb <- lgb.Dataset(data = as.matrix(train_data[, -5]), label = train_data$Species)
test_data_lgb <- lgb.Dataset(data = as.matrix(test_data[, -5]), label = test_data$Species, reference = train_data_lgb)
# LightGBM 모델 파라미터 설정
params <- list(
objective = "multiclass",
num_class = 3,
metric = "multi_logloss",
boosting_type = "gbdt",
num_leaves = 31,
learning_rate = 0.05,
feature_fraction = 0.9,
verbose_eval = TRUE
)
# 모델 훈련
model <- lgb.train(
params = params,
data = train_data_lgb,
nrounds = 100,
valids = list(test = test_data_lgb),  # 검증 데이터에 이름을 지정
early_stopping_rounds = 10
)
# 예측 데이터 준비
predict_data <- as.matrix(test_data[, -5])
# 예측
predictions <- predict(model, data = predict_data)
predicted_labels <- max.col(predictions) - 1
# 정확도 계산
accuracy <- mean(predicted_labels == test_data$Species)
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model)
# 라벨 값 수정 (0, 1, 2로 변경)
iris$Species <- as.numeric(factor(iris$Species)) - 1
# 트레이닝 데이터와 테스트 데이터 분리
set.seed(123)
indices <- sample(1:nrow(iris), nrow(iris) * 0.7)
train_data <- iris[indices, ]
# LightGBM 데이터셋으로 변환
train_data_lgb <- lgb.Dataset(data = as.matrix(train_data[, -5]), label = train_data$Species)
test_data_lgb <- lgb.Dataset(data = as.matrix(test_data[, -5]), label = test_data$Species, reference = train_data_lgb)
# LightGBM 모델 파라미터 설정
params <- list(
objective = "multiclass",
num_class = 3,
metric = "multi_logloss",
boosting_type = "gbdt",
num_leaves = 31,
learning_rate = 0.05,
feature_fraction = 0.9,
verbose_eval = TRUE
)
# 모델 훈련
model <- lgb.train(
params = params,
data = train_data_lgb,
nrounds = 100,
valids = list(test = test_data_lgb),  # 검증 데이터에 이름을 지정
early_stopping_rounds = 10
)
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model)
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model, data_name = "test")
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model, eval_name = "eval")
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model, data_name = "test", eval_name = "eval")
# 검증 데이터에 대한 평가 결과 가져오기
eval_result <- lgb.get.eval.result(model, eval_name = "eval_0")
# 검증 데이터에 대한 평가 결과 확인
valid_results <- model$eval
print(valid_results)
?lightgbm
cat("Accuracy:", accuracy, "\n")
length(test_data)
nrows(train_data)
nrow(train_data)
nrow(test_data)
View(test_data)
install.packages('lightgbm')
# LightGBM 모델 예제
library(lightgbm)
library(dplyr)
# LightGBM 모델 예제
library(lightgbm)
# XGBoost 모델 예제
install.packages('xgboost')
library(xgboost)
help("lightgbm")
# 데이터 불러오기
data(bank, pakage = 'lightgbm')
# 데이터 불러오기
data(bank, package = 'lightgbm')
dim(bank)
head(bank)
head(bank)
table(sum(is.na(bank)))
sum(is.na(bank$age))
sum(is.na(bank$job))
sum(is.na(bank$marital))
sum(is.na(bank$education))
sum(is.na(bank))
# 결측치 확인
colSums(is.na(bank))
str(bank)
# 사본 dt_xgboost 생성
dt_xgboost <- bank    # 원본 데이터를 변경하지 않기 위해 복사
dt_xgboost$y <- ifelse(dt_xgboost$y == no, 0, 1)  # Target변수를 0과 1로 변환
str(dt_xgboost$y)
dt_xgboost$y <- ifelse(dt_xgboost$y == 'no', 0, 1)  # Target변수를 0과 1로 변환
str(dt_xgboost$y)
head(bank, 14)
head(dt_xgboost$y, 14)
table(dt_xgboost$y)
install.packages('Matrix') # sparse matrix 생성에 필요한 패키지 설치, 이미 설치되어 있으면 생략
install.packages("Matrix")
library(Matrix) # Matrix 패키지 로드
# sparse matrix 생성
dt_xgb_sparse_matrix <- sparse.model.matrix(y~. -1, data = dt_xgboost)
# train data set sampling index 정의
train_index <- sample(1:nrow(dt_xgb_sparse_matrix), 2500)
# train 및 test data set 및 label data 생성
train_x <- dt_xgb_sparse_matrix[train_index,]
test_x <- dt_xgb_sparse_matrix[-train_index,]
train_y <- dt_xgboost[train_index,'y']
test_y <- dt_xgboost[-train_index,'y']
dim(train)
dim(train_x)
dim(test_x)
## xgboost 알고리즘 사용을 위한 데이터 형태 변환
dtrain <- xgb.DMatrix(data = train_x, label= as.matrix(train_y))
library(xgboost)
## xgboost 알고리즘 사용을 위한 데이터 형태 변환
dtrain <- xgb.DMatrix(data = train_x, label= as.matrix(train_y))
dtest <- xgb.DMatrix(data = test_x, label= as.matrix(test_y))
# 파라미터 세팅
dt_param <- list(max_depth = 3,
eta = 0.1,
verbose = 0,
nthread = 2,
objective = "binary:logistic",
eval_metric = "auc")
# xgboost 알고리즘 적용
xgb <- xgb.train(params = dt_param,
data = dtrain,
nrounds=10,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 1
)
install.packages("MLmetrics")  # MLmetrics 패키지가 설치되어 있으면 생략 가능
library(MLmetrics)  # MLmetrics 패키지 로드
# 테스트 세트의 확률 값 예측
# train data 확률 값 산출
train_y_pred <- predict(xgb, dtrain)
# test data 확률값 산출
test_y_pred <- predict(xgb, dtest)
# train data에 대한 KS 통계량 산출
KS_Stat(train_y_pred,train_y)
# test data에 대한 KS 통계량 산출
KS_Stat(test_y_pred,test_y)
# 실제 나무가 어떻게 생겼는지....
dt_model <- xgb.dump(xgb, with_stats = T)
dt_model[1:10]   # 모델의 상위 10개 노드를 인쇄합니다.
# 모형에 활용된 feature명 확인
names <- dimnames(dtrain)[[2]]
names
# feature importance matrix 계산
importance_matrix <- xgb.importance(names, model = xgb)
importance_matrix
# feature importance matrix 그래프 그리기
xgb.plot.importance(importance_matrix[1:10,])
new_iris <- lgb.convert_with_rules(data = iris)
library(lightgbm)
new_iris <- lgb.convert_with_rules(data = iris)
str(new_iris$data)
data(iris) # iris dataset 삭제
iris$Species[1L] <- "NEW FACTOR" # Introduce junk factor (NA)
# 알려진 규칙을 사용하여 변환
# 알 수 없는 요인은 0이 되며 희소 데이터 세트에 적합
newer_iris <- lgb.convert_with_rules(data = iris, rules = new_iris$rules)
head(newer_iris$data)
newer_iris$data[1, 5] <- 1.0 # 실제 초기값 되돌리기
rm(iris, new_iris, newer_iris)
new_iris <- lgb.convert_with_rules(data = iris)
str(new_iris$data)
data(iris) # iris dataset 삭제
iris$Species[1L] <- "NEW FACTOR" # Introduce junk factor (NA)
# 알려진 규칙을 사용하여 변환
# 알 수 없는 요인은 0이 되며 희소 데이터 세트에 적합
newer_iris <- lgb.convert_with_rules(data = iris, rules = new_iris$rules)
head(newer_iris$data)
newer_iris$data[1, 5] <- 1.0 # 실제 초기값 되돌리기
# 새로 생성된 데이터 세트가 동일한가요? YES!
all.equal(new_iris$data, newer_iris$data)
# 우리 자신의 규칙을 테스트할 수 있습니까?
data(iris) # Erase iris dataset
# 값을 다르게 재 맵핑
personal_rules <- list(
Species = c(
"setosa" = 3L
, "versicolor" = 2L
, "virginica" = 1L
)
)
newest_iris <- lgb.convert_with_rules(data = iris, rules = personal_rules)
str(newest_iris$data) # 성공
library(lightgbm)
# MLmetrics 패키지 로드
library(MLmetrics)
# LightGBM 패키지에 있는 bank 데이터 가져오기
data(bank, package = "lightgbm")
# 데이터 전처리
data_lightgbm <- bank
data_lightgbm$job <- as.factor(data_lightgbm$job)
data_lightgbm$marital <- as.factor(data_lightgbm$marital)
data_lightgbm$education <- as.factor(data_lightgbm$education)
data_lightgbm$default <- as.factor(data_lightgbm$default)
data_lightgbm$housing <- as.factor(data_lightgbm$housing)
data_lightgbm$loan <- as.factor(data_lightgbm$loan)
data_lightgbm$contact <- as.factor(data_lightgbm$contact)
data_lightgbm$month <- as.factor(data_lightgbm$month)
data_lightgbm$poutcome <- as.factor(data_lightgbm$poutcome)
data_lightgbm$y <- as.factor(data_lightgbm$y)
y <- as.numeric(data_lightgbm$y == "yes")
data_lightgbm_final <- lgb.convert_with_rules(data_lightgbm)
data_lightgbm_final <- lgb.convert_with_rules(data_lightgbm)
# test data set sampling index 정의
train_index <- sample(1:dim(data_lightgbm_final$data)[1], 2000)
# train 및 test data 생성
train_lgbm <- data_lightgbm_final$data[-train_index,]
test_lgbm <- data_lightgbm_final$data[train_index,]
# train data 숫자형 변환
x_train <- as.matrix(train_lgbm[,-"y"])
# train data target 세팅
y_train <- y[-train_index]
# test data 숫자형 변환
x_test <- as.matrix(test_lgbm[,-"y"])
# test data target 세팅
y_test <- y[train_index]
# LightGBM 적용
# -----------------------------------------------------------
dtrain <- lgb.Dataset(x_train, label = y_train)
dtest <- lgb.Dataset.create.valid(dtrain, x_test, label = y_test)
# LightGBM 적용
# -----------------------------------------------------------
dtrain <- lgb.Dataset(x_train, label = y_train)
dtest <- lgb.Dataset.create.valid(dtrain, x_test, label = y_test)
bank_params <- list(
objective = "binary"
, metric = "auc"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = dtest)
banl_model <- lgb.train(
params = bank_params
, data = dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
rm(banl_model)
bank_model <- lgb.train(
params = bank_params
, data = dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
# predict(<lgb.Booster>): LightGBM 모델에 대한 예측 방법
# lgb.Booster 클래스를 기반으로 한 예측 값
preds <- predict(bank_model, x_test)
preds
# lgb.cv(): Cross validation logic used by LightGBM
bank2_model <- lgb.cv( params = bank_params , data = dtrain , nrounds = 5 , nfold = 3 )
# lgb.get.eval.result(): 부스터로부터 평가 결과 얻기
# 유효한 data_name 값 검사
print(setdiff(names(bank_model$record_evals), "start_iter"))
# 데이터세트 "test"에 대한 유효한 eval_name 값 검사
print(names(bank_model$record_evals[["test"]]))
# "test" 데이터 세트에 대한 auc 값 가져오기
lgb.get.eval.result(model, "test", "auc") #
# "test" 데이터 세트에 대한 auc 값 가져오기
lgb.get.eval.result(bank_model, "test", "auc") #
# lgb.get.eval.result(): 부스터로부터 평가 결과 얻기
# 유효한 data_name 값 검사
print(setdiff(names(bank2_model$record_evals), "start_iter")) # [1] "test"
# 데이터세트 "test"에 대한 유효한 eval_name 값 검사
print(names(bank2_model$record_evals[["test"]])) # [1] "auc"
# lgb.cv(): Cross validation logic used by LightGBM
bank_model <- lgb.cv( params = bank_params , data = dtrain , nrounds = 5 , nfold = 3 )
# lgb.get.eval.result(): 부스터로부터 평가 결과 얻기
# 유효한 data_name 값 검사
print(setdiff(names(bank_model$record_evals), "start_iter")) # [1] "test"
# 데이터세트 "test"에 대한 유효한 eval_name 값 검사
print(names(bank_model$record_evals[["test"]])) # [1] "auc"
# "test" 데이터 세트에 대한 auc 값 가져오기
lgb.get.eval.result(bank_model, "test", "auc") # [1] 0.8278460 0.8345746 0.8348805 0.8429948 0.8483243 0.8481459
bank_model <- lgb.train(
params = bank_params
, data = dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
# lgb.cv(): Cross validation logic used by LightGBM
bank2_model <- lgb.cv( params = bank_params , data = dtrain , nrounds = 5 , nfold = 3 )
# lgb.get.eval.result(): 부스터로부터 평가 결과 얻기
# 유효한 data_name 값 검사
print(setdiff(names(bank_model$record_evals), "start_iter")) # [1] "test"
# 데이터세트 "test"에 대한 유효한 eval_name 값 검사
print(names(bank_model$record_evals[["test"]])) # [1] "auc"
# "test" 데이터 세트에 대한 auc 값 가져오기
lgb.get.eval.result(bank_model, "test", "auc") # [1] 0.8278460 0.8345746 0.8348805 0.8429948 0.8483243 0.8481459
# lgb.importance(): Compute feature importance in a model
# 모델의 feature importance에 대한 data.table을 생성합니다.
tree_imp1 <- lgb.importance(bank_model, percentage = TRUE)
tree_imp1
tree_imp2 <- lgb.importance(bank_model, percentage = FALSE)
tree_imp2
# lgb.interprete(): Compute feature contribution of prediction
# 원시 점수 예측(rawscore prediction)의 기능 기여 구성 요소(feature contribution components)를 계산합니다.
tree_interpretation <- lgb.interprete(bank_model, x_test, 1:4)
tree_interpretation
# lgb.plot.importance(): feature importance를 막대 그래프로 표시
# 위에서 계산된 feature importance(Gain, Cover 및 Frequency)를 막대그래프로 표시합니다.
tree_imp <- lgb.importance(bank_model, percentage = TRUE)
lgb.plot.importance(tree_imp, top_n = 5, measure = "Gain")
lgb.plot.interpretation( tree_interpretation_dt = tree_interpretation[[1]] , top_n = 3 )
# lgb.plot.interpretation(): Plot feature contribution as a bar graph
# 이전에 계산된 feature contribution을 막대 그래프로 플로팅 합니다
tree_interpretation <- lgb.interprete( model = model , data = x_test , idxset = 1:4 )
# lgb.plot.interpretation(): Plot feature contribution as a bar graph
# 이전에 계산된 feature contribution을 막대 그래프로 플로팅 합니다
tree_interpretation <- lgb.interprete( model = bank_model , data = x_test , idxset = 1:4 )
tree_interpretation
lgb.plot.interpretation( tree_interpretation_dt = tree_interpretation[[1]] , top_n = 3 )
library(xgboost)               # xgboost 패키지 로딩
library(car)                # car 패키지 로드
dt_xgboost <- bank    # 원본 데이터를 변경하지 않기 위해 복사
dt_xgboost$y <- recode(dt_xgboost$y, "'no'=0; 'yes'=1")  # Target변수를 0과 1로 변환
dt_xgboost$y <- elseif(dt_xgboost$y == 'no', 0 ,1)  # Target변수를 0과 1로 변환
dt_xgboost$y <- ifelse(dt_xgboost$y == 'no', 0 ,1)  # Target변수를 0과 1로 변환
str(dt_xgboost$y)
table(dt_xgboost$y)
library(Matrix)      # Matrix 패키지 로드
# sparse matrix 생성
dt_xgb_sparse_matrix <- sparse.model.matrix(y~. -1, data = dt_xgboost)
# train data set sampling index 정의
train_index <- sample(1:nrow(dt_xgb_sparse_matrix), 2500)
# train 및 test data set 및 label data 생성
train_x <- dt_xgb_sparse_matrix[train_index,]
test_x <- dt_xgb_sparse_matrix[-train_index,]
train_y <- dt_xgboost[train_index,'y']
test_y <- dt_xgboost[-train_index,'y']
dim(train_x)
# [1] 2500   43
dim(test_x)
## xgboost 알고리즘 사용을 위한 데이터 형태 변환
dtrain <- xgb.DMatrix(data = train_x, label= as.matrix(train_y))
dtest <- xgb.DMatrix(data = test_x, label= as.matrix(test_y))
# 파라미터 세팅
dt_param <- list(max_depth = 3,
eta = 0.1,
verbose = 0,
nthread = 2,
objective = "binary:logistic",
eval_metric = "auc")
# xgboost 알고리즘 적용
xgb <- xgb.train(params = dt_param,
data = dtrain,
nrounds=10,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 1
)
# 테스트 세트의 확률 값 예측
# train data 확률 값 산출
train_y_pred <- predict(xgb, dtrain)
# test data 확률값 산출
test_y_pred <- predict(xgb, dtest)
library(MLmetrics)  # MLmetrics 패키지 로드
# train data에 대한 KS 통계량 산출
KS_Stat(train_y_pred,train_y)
# test data에 대한 KS 통계량 산출
KS_Stat(test_y_pred,test_y)
# train data에 대한 KS 통계량 산출
KS_Stat(train_y_pred,train_y)
# 테스트 세트의 확률 값 예측
# train data 확률 값 산출
train_y_pred <- predict(xgb, dtrain)
# test data 확률값 산출
test_y_pred <- predict(xgb, dtest)
# train data에 대한 KS 통계량 산출
KS_Stat(train_y_pred,train_y)
# test data에 대한 KS 통계량 산출
KS_Stat(test_y_pred,test_y)
# 파라미터 세팅
dt_param <- list(max_depth = 3,
eta = 0.1,
verbose = 0,
nthread = 2,
objective = "binary:logistic",
eval_metric = "auc")
# xgboost 알고리즘 적용
xgb <- xgb.train(params = dt_param,
data = dtrain,
nrounds=10,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 1
)
# 테스트 세트의 확률 값 예측
# train data 확률 값 산출
train_y_pred <- predict(xgb, dtrain)
# test data 확률값 산출
test_y_pred <- predict(xgb, dtest)
# train data에 대한 KS 통계량 산출
KS_Stat(train_y_pred,train_y)
# test data에 대한 KS 통계량 산출
KS_Stat(test_y_pred,test_y)
names
dt_model[1:10]   # 모델의 상위 10개 노드를 인쇄합니다.
importance_matrix
# feature importance matrix 그래프 그리기
xgb.plot.importance(importance_matrix[1:10,])
# Target변수를 0과 1로 변환하기
dt_xgboost <- bank    # 원본 데이터를 변경하지 않기 위해 복사
dt_xgboost$y <- ifelse(dt_xgboost$y == 'no', 0 ,1)  # Target변수를 0과 1로 변환
# sparse matrix 생성
dt_xgb_sparse_matrix <- sparse.model.matrix(y~. -1, data = dt_xgboost)
# train data set sampling index 정의
train_index <- sample(1:nrow(dt_xgb_sparse_matrix), 2500)
# train 및 test data set 및 label data 생성
train_x <- dt_xgb_sparse_matrix[train_index,]
test_x <- dt_xgb_sparse_matrix[-train_index,]
train_y <- dt_xgboost[train_index,'y']
test_y <- dt_xgboost[-train_index,'y']
## xgboost 알고리즘 사용을 위한 데이터 형태 변환
dtrain <- xgb.DMatrix(data = train_x, label= as.matrix(train_y))
dtest <- xgb.DMatrix(data = test_x, label= as.matrix(test_y))
# 파라미터 세팅
dt_param <- list(max_depth = 3,
eta = 0.1,
verbose = 0,
nthread = 2,
objective = "binary:logistic",
eval_metric = "auc")
# xgboost 알고리즘 적용
xgb <- xgb.train(params = dt_param,
data = dtrain,
nrounds=10,
subsample = 0.5,
colsample_bytree = 0.5,
num_class = 1
)
# 테스트 세트의 확률 값 예측
# train data 확률 값 산출
train_y_pred <- predict(xgb, dtrain)
# test data 확률값 산출
test_y_pred <- predict(xgb, dtest)
# train data에 대한 KS 통계량 산출
KS_Stat(train_y_pred,train_y)
# test data에 대한 KS 통계량 산출
KS_Stat(test_y_pred,test_y)
# LightGBM 패키지 로드
library(lightgbm)
# MLmetrics 패키지 로드
library(MLmetrics)
# test data set sampling index 정의
cca1_train_index <- sample(1:dim(data_lightgbm_final$data)[1], 0.7)
dim(cca1)
#########################################################
# 사본 cca1 lightGBM 분석 테스트
cca1 <- a1
head(train_lgbm)
head(test_lgbm)
str(train_lgbm)
str(test_lgbm)
str(x_train)
head(x_train)
head(y_train)
str(y_train)
str(x_test)
head(x_test)
str(y_test)
head(y_test)
head(dtrain)
str(dtrain)

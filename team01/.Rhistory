params = params,
data = iris_train_data,
nrounds = 100,  # 학습 라운드 수
verbose = 1
)
# 예측
predictions <- predict(model, as.matrix(features))
# 예측 결과 확인
head(predictions)
# 데이터 로드
data(iris)
data <- iris
# 타겟 변수와 특성 변수 분리
target <- data$Species
features <- data[, -5]  # Species 변수 제외
# LightGBM 데이터셋 생성
iris_train_data <- lgb.Dataset(data = as.matrix(features), label = as.numeric(target) - 1)
# LightGBM 모델 파라미터 설정
params <- list(
objective = "multiclass",
num_class = 3,  # 분류 클래스 수
metric = "multi_logloss"  # 손실함수
)
# 모델 학습
model <- lgb.train(
params = params,
data = iris_train_data,
nrounds = 100,  # 학습 라운드 수
verbose = 1
)
# 예측
predictions <- predict(model, as.matrix(features))
# 예측 결과 확인
head(predictions)
# 조기 종료 및 교차 검증 설정
cv_results <- lgb.cv(
params = params,
data = train_data,
nrounds = 1000,  # 임의의 큰 라운드 수 지정
early_stopping_rounds = 50,  # 조기 종료 판단을 위한 라운드 수
stratified = TRUE,  # 계층적 샘플링
shuffle = TRUE,  # 데이터를 무작위로 섞음
num_threads = 2  # 사용할 스레드 수
)
# 조기 종료 및 교차 검증 설정
cv_results <- lgb.cv(
params = params,
data = iris_train_data,
nrounds = 1000,  # 임의의 큰 라운드 수 지정
early_stopping_rounds = 50,  # 조기 종료 판단을 위한 라운드 수
stratified = TRUE,  # 계층적 샘플링
shuffle = TRUE,  # 데이터를 무작위로 섞음
num_threads = 2  # 사용할 스레드 수
)
# 필요한 라이브러리 로드
library(lightgbm)
library(dplyr)
# 트레이닝 데이터와 테스트 데이터 분리
set.seed(123)
indices <- sample(1:nrow(iris), nrow(iris) * 0.7)
iris_train_data <- iris[indices, ]
iris_test_data <- iris[-indices, ]
# LightGBM 데이터셋으로 변환
iris_train_data_lgb <- lgb.Dataset(data = as.matrix(iris_train_data[, -5]), label = iris_train_data$Species)
iris_test_data_lgb <- lgb.Dataset(data = as.matrix(iris_test_data[, -5]), label = iris_test_data$Species, reference = iris_train_data_lgb)
params <- list(
objective = "multiclass",
num_class = 3,
metric = "multi_logloss",
boosting_type = "gbdt",
num_leaves = 31,
learning_rate = 0.05,
feature_fraction = 0.9
)
model <- lgb.train(
params = params,
data = iris_train_data_lgb,
nrounds = 100,
valids = list(iris_test_data_lgb)
)
rm(iris, iris_test_data, iris_train_data, iris_test_data_lgb, iris_train_data_lgb, iris3)
rm(params)
# 기본 회귀분석부터 해보기
# 변수
# 에너지 사용량(power_consumptino), 워킹데이(w_day), 기온(평균), 강수(합계), 습도(평균)
# 데이터
# b5_test(테스트용)
################################################################################
## 데이터셋 생성(1시간대) b5_test2
b5_test2 <- b5 %>%
group_by(hour) %>%
summarise(
pc=mean(power_consumption),
working = mean(holiday == 0),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
library(dplyr)
# 기본 회귀분석부터 해보기
# 변수
# 에너지 사용량(power_consumptino), 워킹데이(w_day), 기온(평균), 강수(합계), 습도(평균)
# 데이터
# b5_test(테스트용)
################################################################################
## 데이터셋 생성(1시간대) b5_test2
b5_test2 <- b5 %>%
group_by(hour) %>%
summarise(
pc=mean(power_consumption),
working = mean(holiday == 0),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
b5_test2$hoilday <- ifelse(b5_test2$hour >= 6 & b5_test2$hour < 20, 1, 0)
b5_test2
cor(b5_test2$pc, b5_test2$hoilday) # => 0.7911621
cor(b5_test2$pc, b5_test2$temp) # => 0.8399843
cor(b5_test2$pc, b5_test2$rain) # => 0.1608836
cor(b5_test2$pc, b5_test2$humi) # => -0.8321016
b5_glm2 <- lm(pc~., data=b5_test2)
summary(b5_glm2)
step(b5_glm2, direction='both')
b5_glm2 <- lm(pc~hour + temp + rain + w_day, data=b5_test2)
b5_glm2 <- lm(pc~hour + temp + rain + hoilday, data=b5_test2)
summary(b5_glm2)
# 기본 회귀분석부터 해보기
# 변수
# 에너지 사용량(power_consumptino), 워킹데이(w_day), 기온(평균), 강수(합계), 습도(평균)
# 데이터
# b5_test(테스트용)
################################################################################
## 데이터셋 생성(1시간대) b5_test2
b5_test2 <- b5 %>%
group_by(hour) %>%
summarise(
pc=mean(power_consumption),
working = mean(holiday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
b5_test2$hoilday <- ifelse(b5_test2$hour >= 6 & b5_test2$hour < 20, 1, 0)
b5_test2
cor(b5_test2$pc, b5_test2$hoilday) # => 0.7911621
cor(b5_test2$pc, b5_test2$temp) # => 0.8399843
cor(b5_test2$pc, b5_test2$rain) # => 0.1608836
cor(b5_test2$pc, b5_test2$humi) # => -0.8321016
b5_glm2 <- lm(pc~., data=b5_test2)
summary(b5_glm2)
step(b5_glm2, direction='both')
b5_glm2 <- lm(pc~hour + temp + rain + hoilday, data=b5_test2)
summary(b5_glm2)
b5_test1 <- b5 %>%
group_by(date_time2) %>%
summarise(
pc=mean(power_consumption),
working = mean(hoilday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
b5_test1 <- b5 %>%
group_by(date_time) %>%
summarise(
pc=mean(power_consumption),
working = mean(hoilday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
b5_test1 <- b5 %>%
group_by(date) %>%
summarise(
pc=mean(power_consumption),
working = mean(hoilday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
b5_test1 <- b5 %>%
group_by(date) %>%
summarise(
pc=mean(power_consumption),
working = mean(holiday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
# 상관분석
cor(b5_test1$pc, b5_test1$holiday) # => 0.2285323
b5_test2$holiday <- ifelse(b5_test2$hour >= 6 & b5_test2$hour < 20, 1, 0)
b5_test2
cor(b5_test2$pc, b5_test2$holiday) # => 0.7911621
cor(b5_test2$pc, b5_test2$temp) # => 0.8399843
cor(b5_test2$pc, b5_test2$rain) # => 0.1608836
cor(b5_test2$pc, b5_test2$humi) # => -0.8321016
b5_glm2 <- lm(pc~., data=b5_test2)
summary(b5_glm2)
step(b5_glm2, direction='both')
b5_glm2 <- lm(pc~hour + temp + rain + hoilday, data=b5_test2)
b5_test1 <- b5 %>%
group_by(date) %>%
summarise(
pc=mean(power_consumption),
working = mean(holiday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
# 상관분석
cor(b5_test1$pc, b5_test1$holiday) # => 0.2285323
b5_test1 <- b5 %>%
group_by(date) %>%
summarise(
pc=mean(power_consumption),
working = mean(holiday == 1),
temp=mean(temperature),
rain=sum(rainfall),
humi=mean(humidity)
)
rm(b5_test2$hoilday)
b5_test2 <- b5_test2[, -7]
str(a1)
cca1 <- a1
# LightGBM 패키지 로드
library(lightgbm)
# MLmetrics 패키지 로드
library(MLmetrics)
xgboost
library(xgboost)               # xgboost 패키지 로딩
# LightGBM 패키지에 있는 bank 데이터 가져오기
data(bank, package = "lightgbm")
dim(bank) # 4521개의 데이터와 17개의 변수 보유
head(bank) # 데이터 구조 확인
table(sum(is.na(bank))) # 결측 데이터(NA) 유무 확인
colSums(sum(is.na(bank))) # 결측 데이터(NA) 유무 확인
colSums(is.na(bank)) # 결측 데이터(NA) 유무 확인
# 범주형 변수와 숫자형 변수 확인
str(bank)
dim(cca1)
# test data set sampling index 정의
cca1_train_index <- sample(1:dim(cca1$data)[1], nrow(cca1) * 0.7)
# test data set sampling index 정의
cca1_train_index <- sample(1:dim(cca1)[1], nrow(cca1) * 0.7)
# train 및 test data 생성
cca1_train_lgbm <- cca1[-cca1_train_index,]
cca1_test_lgbm <- cca1[cca1_train_index,]
dim(cca1_test_lgbm)
dim(cca1_train_lgbm)
str(cca1)
head(cca1)
# train data 숫자형 변환
cca1_x_train <- as.matrix(cca1_train_lgbm[,-'power_consumption'])
# train data 숫자형 변환
cca1_x_train <- as.matrix(cca1_train_lgbm[,-"power_consumption"])
# train data 숫자형 변환
cca1_x_train <- as.matrix(cca1_train_lgbm[,-c("power_consumption")])
# train data 숫자형 변환
cca1_x_train <- as.matrix(cca1_train_lgbm[,-7])
head(cca1_x_train)
# train data target 세팅
cca1_y_train <- y[-cca1_train_index]
# train data target 세팅
cca1_y_train <- power_consumption[-cca1_train_index]
# train data target 세팅
cca1_y_train <- cca1[-cca1_train_index]
head(cca1_y_train)
y<-as.numeric(cca1$power_consumption)
# train data target 세팅
cca1_y_train <- y[-cca1_train_index]
# test data target 세팅
cca1_y_test <- y[train_index]
# test data target 세팅
cca1_y_test <- y[cca1_train_index]
head(cca1_y_train)
train(cca1_y_test)
str(cca1_y_train)
help(params)
# LightGBM 적용
# -----------------------------------------------------------
cca1_dtrain <- lgb.Dataset(cca1_x_train, label = cca1_y_train)
# LightGBM 패키지 로드
library(lightgbm)
# MLmetrics 패키지 로드
library(MLmetrics)
# LightGBM 적용
# -----------------------------------------------------------
cca1_dtrain <- lgb.Dataset(cca1_x_train, label = cca1_y_train)
cca1_dtest <- lgb.Dataset.create.valid(dtrain, cca1_x_test, label = cca1_y_test)
cca1_dtest <- lgb.Dataset.create.valid(cca1_dtrain, cca1_x_test, label = cca1_y_test)
# test data 숫자형 변환
cca1_x_test <- as.matrix(cca1_test_lgbm[,-7])
# test data target 세팅
cca1_y_test <- y[cca1_train_index]
# LightGBM 적용
# -----------------------------------------------------------
cca1_dtrain <- lgb.Dataset(cca1_x_train, label = cca1_y_train)
cca1_dtest <- lgb.Dataset.create.valid(cca1_dtrain, cca1_x_test, label = cca1_y_test)
cca1_params <- list(
objective = "multiclass"
, metric = "multi_logloss"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
cca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
cca1_params <- list(
objective = "multiclass"
, num_class = 7
, metric = "multi_logloss"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
cca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
cca1_params <- list(
objective = "multiclass"
, num_class = 2490
, metric = "multi_logloss"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
cca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
head(cca1_dtrain)
head(cca1_dtrain$label)
cca1_params <- list(
objective = "multiclass"
, num_class = 7
, metric = "multi_logloss"
, min_data = 50
, learning_rate = 0.3
)
# 레이블을 0부터 6까지의 정수 값으로 변환
cca1_dtrain$label <- cca1_dtrain$label - 1
cca1_params <- list(
objective = "multiclass"
, num_class = 7
, metric = "auc"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
head(cca1_dtrain$label)
# LightGBM 패키지 로드
library(lightgbm)
# MLmetrics 패키지 로드
library(MLmetrics)
install.packages('MLmetrics')
# MLmetrics 패키지 로드
library(MLmetrics)
fcca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
cca1_params <- list(
objective = "multiclass"
, num_class = 7
, metric = "auc"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
head(cca1_dtrain$label)
fcca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
# LightGBM 적용
# -----------------------------------------------------------
cca1_dtrain <- lgb.Dataset(cca1_x_train, label = cca1_y_train)
colSums(is.na(cca1_x_test))
colSums(is.na(cca1_y_test))
colSums(is.na(cca1_x_train))
# LightGBM 적용
# -----------------------------------------------------------
cca1_dtrain <- lgb.Dataset(cca1_x_train, label = cca1_y_train)
cca1_dtest <- lgb.Dataset.create.valid(cca1_dtrain, cca1_x_test, label = cca1_y_test)
cca1_params <- list(
objective = "multiclass"
, num_class = 7
, metric = "auc"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
fcca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
cca1_params <- list(
objective = "multiclass"
, num_class = 7
, metric = "multi_logloss"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
fcca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
cca1_params <- list(
objective = "multiclass"
, metric = "multi_logloss"
, min_data = 50
, learning_rate = 0.3
)
valids <- list(test = cca1_dtest)
fcca1_model <- lgb.train(
params = cca1_params
, data = cca1_dtrain
, nrounds = 10
, valids = valids
, early_stopping_rounds = 1
)
## 데이터 불러오기
rtrain = read.csv('C:/k_digital/r/source/team_01/train.csv')
## 데이터 불러오기
rtrain = read.csv('C:/k_digital/r/source/team01/train.csv')
rtest = read.csv('C:/k_digital/r/source/team01/test.csv')
str(rtrain)
str(rtest)
colSum(is.na(rtrain))
colSums(is.na(rtrain))
colSums(is.na(rtest))
rtest$X
rtrain <- rtrain[, -1]
rtest <- rtest[, -1]
str(rtrain)
str(rtest)
library(dplyr)
library(xgboost)
# 아무거나 선택
high_corr_features <- c('Temp','WS', 'HM', 'WC')
names(rtrain)
# 아무거나 선택
high_corr_features <- c('Temp','WS', 'HM', 'DI','WC')
rtrain <- rtrain %>%
select(BID, Temp, Rain, WS, HM, PC, SR, DI, WC)
# train/test 데이터 분할
set.seed(123)
splitIndex <- createDataPartition(rtrain$PC, p = 0.8, list = FALSE)
train_data <- rtrain[splitIndex, ]
train_data2 <- train_data %>%
select(!PC)
test_data <- rtrain[-splitIndex, ]
library(caret)
splitIndex <- createDataPartition(rtrain$PC, p = 0.8, list = FALSE)
train_data <- rtrain[splitIndex, ]
train_data2 <- train_data %>%
select(!PC)
test_data <- rtrain[-splitIndex, ]
test_data2 <- test_data %>%
select(!PC)
# 데이터 변환
dtrain <- xgb.DMatrix(data = as.matrix(train_data2[, high_corr_features]), label = train_data$PC)
dtest <- xgb.DMatrix(data = as.matrix(test_data2[, high_corr_features]), label = test_data$PC)
# xgboost 파라미터 설정
params <- list(
objective = "reg:squarederror",
eta = 0.1,
max_depth = 60
)
# 모델 학습
bst_model <- xgb.train(params, dtrain, nrounds = 100)
# 예측
predictions <- predict(bst_model, dtest)
# 성능 측정: RMSE
rmse <- sqrt(mean((test_data$PC - predictions)^2))
print(paste("RMSE: ", rmse))
# 성능 측정 : SMAPE
smape <- function(actual, forecast) {
return(100 * mean(2 * abs(forecast - actual) / (abs(actual) + abs(forecast))))
}
preds <- predict(bst_model, dtest)
smape_value <- smape(test_data$PC, preds)
print(paste("SMAPE: ", smape_value))
library(data.table)
load("C:/k_digital/r/source/team01/.RData")

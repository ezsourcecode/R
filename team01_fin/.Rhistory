## library
library(dplyr)
library(kknn)
library(e1071)
library(neuralnet)
library(Metrics)
install.packages("neuralnet")
install.packages("Metrics") # 평가 지표 계산 calculate_errors(actual, forecast) 함수
library(neuralnet)
library(Metrics)
library(gbm)
train <- read.csv("https://raw.githubusercontent.com/Eungyum/Power_Consumption/main/train.csv")
test <- read.csv("https://raw.githubusercontent.com/Eungyum/Power_Consumption/main/test.csv")
train$X <- NULL
test$X <- NULL
## 스코어 저장소 생성
score_LR <- data.frame(BID = NA,SMAPE = vector("numeric", 100), RMSE = vector("numeric", 100),MAE = vector("numeric", 100),MSE = vector("numeric", 100))
score_SVM <- data.frame(BID = 1:100,SMAPE = vector("numeric", 100),RMSE = vector("numeric", 100),MAE = vector("numeric", 100),MSE = vector("numeric", 100))
score_kNN <- data.frame(BID = 1:100,SMAPE = vector("numeric", 100),RMSE = vector("numeric", 100),MAE = vector("numeric", 100),MSE = vector("numeric", 100))
score_NN <- data.frame(BID = 1:100,SMAPE = vector("numeric", 100),RMSE = vector("numeric", 100),MAE = vector("numeric", 100),MSE = vector("numeric", 100))
score_gbm <- data.frame(BID = 1:100,SMAPE = vector("numeric", 100),RMSE = vector("numeric", 100),MAE = vector("numeric", 100),MSE = vector("numeric", 100))
# 오차 측정 함수 정의
calculate_errors <- function(actual, predicted) {
# SMAPE (Symmetric Mean Absolute Percentage Error)
smape <- 100 * mean(2 * abs(predicted - actual) / (abs(predicted) + abs(actual)), na.rm = TRUE)
# RMSE (Root Mean Squared Error)
rmse <- rmse(actual, predicted)
# MAE (Mean Absolute Error)
mae <- mae(actual, predicted)
# MSE (Mean Squared Error)
mse <- mse(actual, predicted)
return(data.frame(BID = i, SMAPE = smape, RMSE = rmse, MAE = mae, MSE = mse))
}
for (i in 1:100) {
# 훈련세트 : train_LR
train_LR <-train %>%
filter(BID==i)
# 테스트 세트 : test_LR
test_LR <- train %>%
filter(BID==i)
# 모델 생성
model_LR <- lm(PC~Temp+Rain+WS+HM+WorkD+WC+DI, data=train_LR)
# 스텝와이즈로 최적의 모델 생성(최종 모델)
model_LR <- step(model_LR, direction="both")
# 모델을 저장 : model_LR_i
saveRDS(model_LR, file=paste0("model_LR_", i))
# 실제값 저장 : actual
actual <- test_LR$PC
# 예측값 생성
forecast <- predict(model_LR, test_LR)
#############################################################
## 스코어s ##
score_LR[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for (i in 1:100) {
train_SVM <-train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
test_SVM <- train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
# 모델 생성
model_SVM <- svm(PC~., data=train_SVM, type = "eps-regression", kernel = "linear")
# 모델을 저장 : model_SVM_i
saveRDS(model_SVM, file=paste0("model_SVM_", i))
# 실제값 저장 : actual
actual <- test_SVM$PC
# 예측값 생성
forecast <- predict(model_SVM, test_SVM)
#############################################################
## 스코어s ##
score_SVM[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for(i in 1:100) {
train_kNN <-train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD, WC, DI)
test_kNN <- train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD, WC, DI)
# 모델 생성
model_kNN <- train.kknn(PC ~ ., train_kNN, kmax = 2, kernel = "optimal")
# 모델을 저장 : model_kNN_i
saveRDS(model_kNN, file=paste0("model_kNN_", i))
# 실제값 저장 : actual
actual <- test_kNN$PC
# 예측값 생성 : forecast
forecast <- predict(model_kNN, test_kNN)
#############################################################
## 스코어s ##
score_kNN[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for (i in 1:100) {
## 데이터셋 생성
train_NN <-train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
test_NN <- train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
# 모델 생성
model_NN <- neuralnet(PC~., data=train_NN, hidden=20, linear.output=TRUE) # 은닉층에 10개 뉴런
# 모델을 저장 : model_NN_i
saveRDS(model_NN, file=paste0("model_NN_", i))
# model_NN 요약본
summary(model_NN)
# 실제값 저장 : actual
actual <- test_NN$PC
# 예측값 생성
forecast <- predict(model_NN, test_NN)
#############################################################
## 스코어s ##
score_NN[i,]<-calculate_errors(actual, forecast)
}
View(score_kNN)
df <- data.frame(mean(score_LR$SMAPE), mean(score_SVM$SMAPE), mean(score_gbm$SMAPE), mean(score_kNN$SMAPE),mean(score_NN$SMAPE))
View(df)
# 테스트 세트 : test_LR
test_LR <- test %>%
filter(BID==i)
# 모델 생성
model_LR <- lm(PC~Temp+Rain+WS+HM+WorkD+WC+DI, data=train_LR)
# 스텝와이즈로 최적의 모델 생성(최종 모델)
model_LR <- step(model_LR, direction="both")
# 모델을 저장 : model_LR_i
saveRDS(model_LR, file=paste0("model_LR_", i))
# 실제값 저장 : actual
actual <- test_LR$PC
# 예측값 생성
forecast <- predict(model_LR, test_LR)
score_LR[i,]<-calculate_errors(actual, forecast)
test_SVM <- test %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
# 모델 생성
model_SVM <- svm(PC~., data=train_SVM, type = "eps-regression", kernel = "linear")
# 모델을 저장 : model_SVM_i
saveRDS(model_SVM, file=paste0("model_SVM_", i))
# 실제값 저장 : actual
actual <- test_SVM$PC
# 예측값 생성
forecast <- predict(model_SVM, test_SVM)
score_SVM[i,]<-calculate_errors(actual, forecast)
for (i in 1:100) {
# 훈련세트 : train_LR
train_LR <-train %>%
filter(BID==i)
# 테스트 세트 : test_LR
test_LR <- test %>%
filter(BID==i)
# 모델 생성
model_LR <- lm(PC~Temp+Rain+WS+HM+WorkD+WC+DI, data=train_LR)
# 스텝와이즈로 최적의 모델 생성(최종 모델)
model_LR <- step(model_LR, direction="both")
# 모델을 저장 : model_LR_i
saveRDS(model_LR, file=paste0("model_LR_", i))
# 실제값 저장 : actual
actual <- test_LR$PC
# 예측값 생성
forecast <- predict(model_LR, test_LR)
#############################################################
## 스코어s ##
score_LR[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for (i in 1:100) {
train_SVM <-train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
test_SVM <- test %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD)
# 모델 생성
model_SVM <- svm(PC~., data=train_SVM, type = "eps-regression", kernel = "linear")
# 모델을 저장 : model_SVM_i
saveRDS(model_SVM, file=paste0("model_SVM_", i))
# 실제값 저장 : actual
actual <- test_SVM$PC
# 예측값 생성
forecast <- predict(model_SVM, test_SVM)
#############################################################
## 스코어s ##
score_SVM[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for(i in 1:100) {
train_kNN <-train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD, WC, DI)
test_kNN <- test %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD, WC, DI)
# 모델 생성
model_kNN <- train.kknn(PC ~ ., train_kNN, kmax = 2, kernel = "optimal")
# 모델을 저장 : model_kNN_i
saveRDS(model_kNN, file=paste0("model_kNN_", i))
# 실제값 저장 : actual
actual <- test_kNN$PC
# 예측값 생성 : forecast
forecast <- predict(model_kNN, test_kNN)
#############################################################
## 스코어s ##
score_kNN[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for(i in 1:100) {
train_kNN <-train %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD, WC, DI)
test_kNN <- test %>%
filter(BID==i) %>%
select(PC, Temp, Rain, WS, HM, Time, WorkD, WC, DI)
# 모델 생성
model_kNN <- train.kknn(PC ~ ., train_kNN, kmax = 2, kernel = "optimal")
# 모델을 저장 : model_kNN_i
saveRDS(model_kNN, file=paste0("model_kNN_", i))
# 실제값 저장 : actual
actual <- test_kNN$PC
# 예측값 생성 : forecast
forecast <- predict(model_kNN, test_kNN)
#############################################################
## 스코어s ##
score_kNN[i,]<-calculate_errors(actual, forecast)
} # for문 끝
for ( i in 1:100 ){
train_gbm <-train %>%
filter(BID==i)
test_gbm <- test %>%
filter(BID==i)
train_gbm$BID <- NULL
train_gbm$DateTime <- NULL
train_gbm$WeekD <- NULL
train_gbm$DILv <- NULL
test_gbm$BID <- NULL
test_gbm$DateTime <- NULL
test_gbm$WeekD <- NULL
test_gbm$DILv <- NULL
# 모델 생성
model_gbm <- gbm(PC ~ ., data=train_gbm, distribution="gaussian", n.trees = 100, interaction.depth = 5,
shrinkage = 0.05, n.minobsinnode=23)
# 모델을 저장 : model_kNN_i
saveRDS(model_gbm, file=paste0("model_gbm_", i))
# 실제값 저장 : actual
actual <- test_gbm$PC
# 예측값 생성 : forecast
forecast <- predict(model_gbm, test_gbm)
#############################################################
## 스코어s ##
score_gbm[i,]<-calculate_errors(actual, forecast)
} # for문 끝
View(score_gbm)
View(score_kNN)
df <- data.frame(mean(score_LR$SMAPE), mean(score_SVM$SMAPE), mean(score_gbm$SMAPE), mean(score_kNN$SMAPE))
View(df)
library(ggplot2)
barplot(df)
ggplot(df)
barplot(df,names=c("LR","SVM","GBM", "KNN"))
str(df)
df <- unlist(df)
barplot(df,names=c("LR","SVM","GBM", "KNN"))
library(xgboost)
library(caret)
barplot(df,names=c("LR","SVM","GBM", "KNN"), col= "purple")
df
